{
 "metadata": {
  "name": "",
  "signature": "sha256:ec45b8ff0c584f98e5d0d3e2f8424f45465781f463945da4acd2ce4c02c20748"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Project 3: Text analysis of political social media\n",
      "======\n",
      "\n",
      "- By Jacob Eisenstein\n",
      "- For CS 8803-CSS, March 2015\n",
      "\n",
      "This project involves mining text from the social media site Reddit. \n",
      "It is possible that you will encounter content that is offensive and derogatory. \n",
      "We are studying this content, not endorsing it. \n",
      "But if you encounter something that is too upsetting for you to work with, please contact me and we will work out a solution."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Setup #\n",
      "\n",
      "This code is written by me, and includes code to load in text that I scraped from Reddit.\n",
      "\n",
      "I'm also including the code that I used to do the scraping (in another file), in case you want to expand this project later."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "import csv\n",
      "from collections import Counter, defaultdict\n",
      "from nltk import sent_tokenize,word_tokenize,porter\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn import cross_validation\n",
      "import codecs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "subreddit_names = ['Libertarian','Conservative','Progressive','Socialism','Anarchism']\n",
      "linebreak = '-----==----==---==-----'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "subreddits = dict()\n",
      "for subreddit in subreddit_names:\n",
      "    comments = []\n",
      "    with codecs.open('.'.join([subreddit,'txt']),'r','utf-8-sig') as fin:\n",
      "        comment = ''        \n",
      "        for line in fin:\n",
      "            if not(line.rstrip() == linebreak):\n",
      "                #print line\n",
      "                #comment += line.decode('utf-8')\n",
      "                comment += line\n",
      "            else:\n",
      "                comments.append(comment)\n",
      "                comment = ''\n",
      "    subreddits[subreddit] = comments"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 60
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is my function for building a dictionary of words from a set of threads.\n",
      "\n",
      "Notice that it takes a function as an argument, which allows you to preprocess the words."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getWordCounts(comments,word_proc = lambda x : x):\n",
      "    word_counts = Counter()\n",
      "    for comment in comments:\n",
      "        for sent in sent_tokenize(comment):\n",
      "            for word in word_tokenize(sent):\n",
      "                word_counts[word_proc(word)] += 1\n",
      "    return word_counts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 61
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This function makes a plot with log rank on the x-axis and log count on the y-axis, for one thread"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def scatterCounts(counts,color='b'):\n",
      "    plt.scatter(np.log(range(len(counts))),[np.log(x[1]) for x in counts.most_common()],marker='.',alpha=0.5,color=color)\n",
      "    plt.xlabel('log rank')\n",
      "    plt.ylabel('log count')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "allcomments = []\n",
      "for subreddit in subreddits.values():\n",
      "    for comment in subreddit:\n",
      "        allcomments.append(comment)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scatterCounts(getWordCounts(allcomments));"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAENCAYAAAAIbA6TAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7febe025f610>"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 1a**\n",
      "\n",
      "Modify the word counter function to downcase and/or stem words, using the Porter Stemmer (see nltk for information).\n",
      "Overlay three plots:\n",
      "\n",
      "- original (blue)\n",
      "- downcase (red)\n",
      "- downcased and stemmed (green)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# your code"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 1b**\n",
      "\n",
      "Please explain the difference between the three lines.\n",
      "\n",
      "In each case, what percentage of the words are singletons, a.k.a. [hapax legomena](http://en.wikipedia.org/wiki/Hapax_legomenon)?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# your code"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 1c** Write a modified version of getWordCounts that returns bigram counts. See nltk.util.bigrams.\n",
      "\n",
      "Make the plots from Deliverable 1a for the bigrams (with raw, downcased, and stemmed bigrams). \n",
      "Compute the percentage of singletones."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.util import bigrams"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# your code\n",
      "# note that the word_proc argument to getWordCounts allows you to specify some preprocessing operations"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 66
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 1d** Compute the frequency of each word in each set, using downcasing but not stemming.\n",
      "\n",
      "- Choose two subreddits (e.g., \"Libertarian\" and \"Conservative\")\n",
      "- Build a vocabulary of all words that appear in both subreddits.\n",
      "- Find the differences in word frequencies between the two subreddits. \n",
      "Print the top 10 and bottom 10 words.\n",
      "- Find the ratios of word frequencies between the two subreddits. Print the top 10 and bottom 10 words."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# your code"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 1e** The Jurafsky et al paper uses a technique from Monroe et al (2008) to score words -- see the equations on the second and third pages of the paper.\n",
      "\n",
      "Reimplement this technique, setting $\\alpha_w = 0.01$ for all words, and $\\alpha_0 = V \\alpha_w$, with $V$ equal to the size of the vocabulary. Show the top words for your comparison groups, ranked by z-score. Explain how this word list relates to the previous two that you have computed."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# your code"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** Extra-credit 1 ** (small) Make a plot as similar as possible to the ones in the Monroe et al paper, e.g. figure 5."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# your code"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Word Lists #\n",
      "\n",
      "In this section, you will use the [General Inquirer](http://www.wjh.harvard.edu/~inquirer) lexicon to learn more about these political subreddits. Download the \"inqtabs.txt\" file at [this URL](http://www.wjh.harvard.edu/~inquirer/inqtabs.txt)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# this code might be helpful to see how to read the file\n",
      "keys_to_ignore = ['Entry','Source','Defined']\n",
      "with open('inqtabs.txt') as fin:\n",
      "    reader = csv.DictReader(fin,delimiter='\\t')\n",
      "    for i,line in enumerate(reader):\n",
      "        print line['Entry'], ' '.join([\"%s:%s\"%(key,val) for key,val in line.iteritems() if len(val)>0 and key not in keys_to_ignore])\n",
      "        if i > 20: \n",
      "            break"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "A Othtags:DET ART\n",
        "ABANDON Weak:Weak Othtags:SUPV IAV:IAV Ngtv:Ngtv Fail:Fail Negativ:Negativ AffTot:AffTot AffLoss:AffLoss\n",
        "ABANDONMENT Weak:Weak Othtags:Noun Fail:Fail Negativ:Negativ\n",
        "ABATE Passive:Passive Othtags:SUPV IAV:IAV TranLw:TranLw Decreas:Decreas Negativ:Negativ\n",
        "ABATEMENT Othtags:Noun\n",
        "ABDICATE Passive:Passive Finish:Finish Weak:Weak Othtags:SUPV IAV:IAV Submit:Submit Negativ:Negativ\n",
        "ABHOR Passive:Passive Othtags:SUPV Hostile:Hostile SV:SV Arousal:Arousal Negativ:Negativ\n",
        "ABIDE Positiv:Positiv Doctrin:Doctrin Othtags:SUPV IAV:IAV Active:Active Affil:Affil\n",
        "ABILITY Strong:Strong Positiv:Positiv Othtags:Noun EVAL:EVAL Virtue:Virtue Abs@:Abs@ MeansLw:MeansLw ABS:ABS\n",
        "ABJECT Passive:Passive Weak:Weak Othtags:Modif IPadj:IPadj Vice:Vice Submit:Submit Negativ:Negativ\n",
        "ABLE Strong:Strong Positiv:Positiv Othtags:Modif EVAL:EVAL Virtue:Virtue Pstv:Pstv MeansLw:MeansLw\n",
        "ABNORMAL Othtags:Modif Ngtv:Ngtv Vice:Vice Negativ:Negativ NegAff:NegAff\n",
        "ABOARD Othtags:PREP LY Space:Space\n",
        "ABOLISH Strong:Strong Othtags:SUPV IAV:IAV Ngtv:Ngtv Hostile:Hostile PowOth:PowOth SocRel:SocRel Active:Active Power:Power Negativ:Negativ PowTot:PowTot\n",
        "ABOLITION Othtags:Noun TranLw:TranLw\n",
        "ABOMINABLE Strong:Strong Othtags:Modif Ovrst:Ovrst Vice:Vice Eval@:Eval@ Negativ:Negativ IndAdj:IndAdj\n",
        "ABORTIVE Othtags:Modif POLIT PowOth:PowOth PowTot:PowTot\n",
        "ABOUND Passive:Passive Positiv:Positiv Othtags:SUPV IAV:IAV Increas:Increas\n",
        "ABOUT#1 Othtags:PREP\n",
        "ABOUT#2 Othtags:PREP LY Quan:Quan Undrst:Undrst If:If\n",
        "ABOUT#3 Othtags:PREP LY Space:Space\n",
        "ABOUT#4 Othtags:SUPV VERB MOD TimeSpc:TimeSpc DAV:DAV\n"
       ]
      }
     ],
     "prompt_number": 76
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 2a** Compute the counts and frequency of positive and negative words in each subreddit, using the positive and negative word categories from the General Inquirer. Note that some words have multiple senses (e.g. \"ABOUT\"). Combine all tags for all senses."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#keep\n",
      "line['Entry'].lower()[:line['Entry'].index('#')]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 77,
       "text": [
        "'about'"
       ]
      }
     ],
     "prompt_number": 77
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# your code"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 2b** Use the binomial CDF to determine whether the **proportion of emotion words** that are positive in each subreddit is **significantly different** from the baseline rate, computed over all other subreddits. (Emotion words are all those words that are marked as either Positive or Negative.) Report a two-tailed p-value. Recall that p-values are always in the range $[0,1]$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.stats import binom"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 81
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# your code"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 2c** Perform a similar analysis for another GI category of your choosing. \n",
      "You may compare the counts of the category against the total number of tokens, or against an alternative category (as done above), but please justify your choice. Compute a two-tailed p-value as above."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# your code"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Classification #\n",
      "\n",
      "In this part, you will build a classifier to predict the correct subreddit for each comment.\n",
      "\n",
      "To do this, you will use [sklearn](http://scikit-learn.org/). You should take a look at the documentation for this package. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# you may find this generator helpful, but you don't have to use it\n",
      "def getText(subreddits):\n",
      "    for subreddit in subreddits:\n",
      "        for comment in subreddit:\n",
      "            yield comment"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 84
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 3a** \n",
      "\n",
      "- Use [CountVectorizer](http://scikit-learn.org/stable/modules/feature_extraction.html) to build a matrix representation of the text, such that each row is a comment and each column is a word. Include all words.\n",
      "- Print the shape of this matrix\n",
      "- Use this matrix to redo the (sorted) log-count-log-rank plot from the previous section, this time over all documents in the dataset.\n",
      "- Also use this matrix to count the percentage of singleton words."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# your code"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 3b** \n",
      "You will now train a [Logistic Regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) classifier. For now, you may use the default parameters.\n",
      "\n",
      "- To do this, you will need a vector or list of labels for each comment. The labels should be numeric. \n",
      "- Create this vector, and use it to compute the proportion of comments from each class.\n",
      "- One you have the vector, apply [cross-validation](http://scikit-learn.org/stable/modules/feature_extraction.html) to evaluate the classifier performance.\n",
      "- Try 3-fold, 4-fold, 5-fold, 7-fold, and 10-fold cross-validation. Plot the results and explain the trend that you observe (explaining why you think it is happening)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# your code"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Logistic regression has a tuning parameter: regularization (called \"C\"). \n",
      "You can set this using [cross-validation](http://scikit-learn.org/stable/auto_examples/grid_search_digits.html).\n",
      "\n",
      "**Deliverable 3c** Try several values for regularization, and plot the average accuracy on 5-fold cross-validation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.grid_search import GridSearchCV"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 91
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# your code"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 3d** Using the best regularization, fit a classifier to the entire dataset.\n",
      "\n",
      "List the top 5 words with the highest coefficients for each class. See clf.coef_ "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# your code"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 3e** Now include bigrams in the training set.\n",
      "\n",
      "- See CountVectorizer for some idea about how to do this.\n",
      "- Use GridSearchCV to find the accuracy across a range of C parameters, with 5 CV folds\n",
      "- Train a classifier using the best C parameter on the entire training set\n",
      "- For each class, report the ten features (unigrams or bigrams) with the highest positive weights in this classifier"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# your code"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 3f: Player's choice** \n",
      "\n",
      "Try to improve the classification accuracy, either by finding another classifier or doing some intelligent preprocessing of the text. This part will count as much as the rest of deliverable 2, so please make a good effort."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# your code"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Topic models #"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** Deliverable 4a** Use CountVectorizer to build a data matrix 'x', where you include all words that appear in at least five documents and fewer than 10% of all documents. Print the shape of this matrix."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# your code"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The code below removes all documents with fewer than 20 tokens."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x_long_comments = x[[i for i,s in enumerate(x.sum(1)) if s > 20],:]\n",
      "print x_long_comments.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'x' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-15-59330c8c6fdf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx_long_comments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mx_long_comments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Gensim ##\n",
      "\n",
      "Gensim is a package for topic modeling in python (among other things). \n",
      "You can install it with \"pip install gensim\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim.models import ldamodel;\n",
      "from gensim import matutils;"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 104
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The code below converts your word matrix to a gensim object"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpus_gensim = matutils.Sparse2Corpus(x_long_comments.T)\n",
      "vocab_gensim = {val:key for key,val in vectorizer.vocabulary_.items()}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 105
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This code runs an LDA topic model on the corpus_gensim data structure, for 100 passes, with 5 topics. Use vocab_gensim as the vocabulary, as specified by the id2word argument. You may want to try with the option \"alpha='auto'\"."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#takes around 5 minutes on my machine\n",
      "model = ldamodel.LdaModel(corpus_gensim,id2word=vocab_gensim,num_topics=5,passes=100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 106
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 4b** Show the top words per topic. Use the following equation:\n",
      "\n",
      "$\\psi_{k,w} = \\log \\beta_{k,w} - \\log \\frac{1}{K} \\sum_{k'} \\beta_{k',w}$,\n",
      "\n",
      "where \n",
      "- $k$ indexes a topic\n",
      "- $w$ indexes a word\n",
      "- $\\psi_{k,w}$ is the score for a word $w$ in topic $k$\n",
      "- $K$ is the total number of topics\n",
      "- $\\beta_{k,w}$ is the probability of word $w$ appearing in topic $k$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# your code"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** Deliverable 4c ** Now compute 15 topics (or more) over the same corpus, again for at least 100 passes. This may take a little while, but it should be less than an hour. Show the topics using the same code as you wrote for deliverable 4d."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# your code"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 4d** Examine the topics. Do you think any of them should be more common in some of the subreddits? \n",
      "\n",
      "To compute the strength of topics in each document, you can use the code below.\n",
      "\n",
      "For your selected topic, use a z-test to determine whether it is indeed more common in the subreddit that you selected."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# your code"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    }
   ],
   "metadata": {}
  }
 ]
}