{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Project 2: Text analysis of political social media\n",
      "-----\n",
      "\n",
      "- By Jacob Eisenstein\n",
      "- For CS 8803-CSS, March 2014\n",
      "\n",
      "This project involves mining text from the social media site Reddit. \n",
      "It is possible that you will encounter content that is offensive and derogatory. \n",
      "We are studying this content, not endorsing it. \n",
      "But if you encounter something that is too upsetting for you to work with, please contact me and we will work out a solution."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Setup #\n",
      "\n",
      "This code is written by me, and includes code to load in text that I scraped from Reddit.\n",
      "\n",
      "I'm also including the code that I used to do the scraping (in another file), in case you want to expand this project later."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json, time, praw, csv, cPickle\n",
      "from datetime import datetime\n",
      "from collections import Counter\n",
      "from nltk import sent_tokenize,word_tokenize,porter\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn import cross_validation"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "subreddit_names = ['Libertarian','Conservative','Progressive','Socialism','Anarchism']\n",
      "linebreak = '-----==----==---==-----'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "subreddits = dict()\n",
      "for subreddit in subreddit_names:\n",
      "    comments = []\n",
      "    with open('.'.join([subreddit,'txt']),'r') as fin:\n",
      "        comment = ''        \n",
      "        for line in fin:\n",
      "            if not(line.rstrip() == linebreak):\n",
      "                #print line\n",
      "                comment += line\n",
      "            else:\n",
      "                comments.append(comment)\n",
      "                comment = ''\n",
      "    subreddits[subreddit] = comments"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 52
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is my function for building a dictionary of words from a set of threads.\n",
      "\n",
      "Notice that it takes a function as an argument, which allows you to preprocess the words."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getWordCounts(comments,word_proc = lambda x : x):\n",
      "    word_counts = Counter()\n",
      "    for comment in comments:\n",
      "        for sent in sent_tokenize(comment):\n",
      "            for word in word_tokenize(sent):\n",
      "                word_counts[word_proc(word)] += 1\n",
      "    return word_counts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 53
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This function makes a plot with log rank on the x-axis and log count on the y-axis, for one thread"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def scatterCounts(counts,color='b'):\n",
      "    plt.scatter(np.log(range(len(counts))),[np.log(x[1]) for x in counts.most_common()],marker='.',alpha=0.5,color=color)\n",
      "    plt.xlabel('log rank')\n",
      "    plt.ylabel('log count')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "allcomments = []\n",
      "for subreddit in subreddits.values():\n",
      "    for comment in subreddit:\n",
      "        allcomments.append(comment)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scatterCounts(getWordCounts(allcomments))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "-c:2: RuntimeWarning: divide by zero encountered in log\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEMCAYAAADJQLEhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x442d390>"
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 1a**\n",
      "\n",
      "Modify the word counter function to downcase and/or stem words, using the Porter Stemmer (see nltk for information).\n",
      "Overlay three plots:\n",
      "\n",
      "- original (blue)\n",
      "- downcase (red)\n",
      "- downcased and stemmed (green)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 1b**\n",
      "\n",
      "Please explain the difference between the three lines.\n",
      "\n",
      "In each case, what percentage of the words are singletons, a.k.a. [hapax legomena](http://en.wikipedia.org/wiki/Hapax_legomenon)?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 1c** Write a modified version of getWordCounts that returns bigram counts. See nltk.util.bigrams.\n",
      "\n",
      "Make the plots from Deliverable 1a for the bigrams (with raw, downcased, and stemmed bigrams). \n",
      "Compute the percentage of singletones."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 1d** Compute the frequency of each word in each set, using downcasing but not stemming.\n",
      "\n",
      "- Choose two subreddits (e.g., \"Libertarian\" and \"Conservative\")\n",
      "- Build a vocabulary of all words that appear in both subreddits.\n",
      "- Find the differences in word frequencies between the two subreddits. \n",
      "Print the top 10 and bottom 10 words.\n",
      "- Find the ratios of word frequencies between the two subreddits. Print the top 10 and bottom 10 words."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Classification #\n",
      "\n",
      "In this part, you will build a classifier to predict the correct subreddit for each comment.\n",
      "\n",
      "To do this, you will use [sklearn](http://scikit-learn.org/). You should take a look at the documentation for this package. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# you may find this generator helpful, but you don't have to use it\n",
      "def getText(subreddits):\n",
      "    for subreddit in subreddits:\n",
      "        for comment in subreddit:\n",
      "            yield comment"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 68
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 2a** \n",
      "\n",
      "- Use [CountVectorizer](http://scikit-learn.org/stable/modules/feature_extraction.html) to build a matrix representation of the text, such that each row is a comment and each column is a word. Include all words.\n",
      "- Print the shape of this matrix\n",
      "- Use this matrix to redo the (sorted) log-count-log-rank plot from the previous section, this time over all documents in the dataset.\n",
      "- Also use this matrix to count the percentage of singleton words."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer = CountVectorizer(yourargumentshere)\n",
      "x = vectorizer.something\n",
      "print x.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(4291, 17139)\n"
       ]
      }
     ],
     "prompt_number": 69
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 2b** \n",
      "You will now train a [Logistic Regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) classifier. For now, you may use the default parameters.\n",
      "\n",
      "- To do this, you will need a vector or list of labels for each comment. The labels should be numeric. \n",
      "- Create this vector, and use it to compute the proportion of comments from each class.\n",
      "- One you have the vector, apply [cross-validation](http://scikit-learn.org/stable/modules/feature_extraction.html) to evaluate the classifier performance.\n",
      "- Try 3-fold, 4-fold, 5-fold, 7-fold, and 10-fold cross-validation. Plot the results and explain the trend that you observe (explaining why you think it is happening).\n",
      "- Sanity check: my accuracy is around 45%"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Logistic regression has a key parameter: regularization (called \"C\"). \n",
      "You can tune this using [cross-validation](http://scikit-learn.org/stable/auto_examples/grid_search_digits.html).\n",
      "\n",
      "**Deliverable 2c** Try several values for regularization, and plot the average accuracy on 5-fold cross-validation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#keep this\n",
      "from sklearn.grid_search import GridSearchCV"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 80
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 2d** Using the best regularization, fit a classifier to the entire dataset.\n",
      "\n",
      "Plot the top 5 words with the highest coefficients for each class. See clf.coef_ "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 2e** Now include bigrams in the training set.\n",
      "\n",
      "- See CountVectorizer for some idea about how to do this.\n",
      "- Use GridSearchCV to find the accuracy across a range of C parameters, with 5 CV folds"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 2f: Player's choice** \n",
      "\n",
      "Try to improve the classification accuracy, either by finding another classifier or doing some intelligent preprocessing of the text. This part will count as much as the rest of deliverable 2, so please make a good effort."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Topic models #"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now use CountVectorizer to build a data matrix 'x', where you include all words that appear in at least two documents and fewer than 10% of all documents."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer = CountVectorizer(yourargumentshere)\n",
      "x = vectorizer.something"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 95
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Sampling ##\n",
      "\n",
      "This is code for sampling from an unnormalized discrete probability distribution. Sampling is the main bottleneck in Gibbs sampling, so it's important for it to be fast. I left in two other things that I tried, in case that's interesting to you.\n",
      "\n",
      "This can be further accelerated by applying substantially more cleverness, as shown in [this paper](www.ics.uci.edu/~newman/pubs/fastlda.pdf\u200e)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def multiSample(probs):\n",
      "    tot = sum(probs)\n",
      "    val = tot * random.random()\n",
      "    i = 0\n",
      "    while val > probs[i]:\n",
      "        val -= probs[i]\n",
      "        i += 1\n",
      "    return i"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def multiSample2(probs):\n",
      "    cumsum = np.cumsum(probs)\n",
      "    val = cumsum[-1] * random.random()\n",
      "    for i,thresh in enumerate(cumsum):\n",
      "        if val < thresh:\n",
      "            return i"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def multiSample3(probs):\n",
      "    cumsum = np.cumsum(probs)\n",
      "    val = cumsum[-1] * random.random()\n",
      "    return sum(val > cumsum)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_p = [0.1,0.5,0.1,0.5,0.001,0.2,0.3,0.05,0.25,0.8]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 98
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%timeit\n",
      "multiSample(test_p)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "10000 loops, best of 3: 93.7 \u00b5s per loop\n"
       ]
      }
     ],
     "prompt_number": 99
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%timeit\n",
      "multiSample2(test_p)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "10000 loops, best of 3: 70.4 \u00b5s per loop\n"
       ]
      }
     ],
     "prompt_number": 100
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%timeit\n",
      "multiSample3(test_p)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "10000 loops, best of 3: 114 \u00b5s per loop\n"
       ]
      }
     ],
     "prompt_number": 101
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Latent Dirichlet Allocation ##\n",
      "\n",
      "Here is my code for LDA, using collapsed Gibbs sampling. Make sure you understand how it works, so you can do the remaining parts of the project. \n",
      "\n",
      "- If you missed class or forget how Collapsed Gibbs Sampling works for standard LDA (implemented above), you can see [these notes](http://people.cs.umass.edu/~wallach/courses/s11/cmpsci791ss/readings/griffiths02gibbs.pdf).\n",
      "- Much more detail is provided [here](http://lingpipe.files.wordpress.com/2010/07/lda3.pdf).\n",
      "- Here is the [original paper](http://people.csail.mit.edu/brussell/research/words/ICCV05/GS04.pdf), also useful."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class LDA():\n",
      "    def __init__(s,text,K,alpha = 1e-1, eta=1e-4):\n",
      "        s.D, s.V = text.shape\n",
      "        s.alpha = alpha\n",
      "        s.eta = eta\n",
      "        s.text = text\n",
      "        s.K = K\n",
      "        s.kw_counts = np.zeros((K,s.V))\n",
      "        s.dk_counts = np.zeros((s.D,K))\n",
      "        s.z = []\n",
      "        for d,v,c in zip(text.row, text.col, text.data):\n",
      "            for _ in range(c):\n",
      "                k = random.randint(s.K)\n",
      "                s.kw_counts[k,v] +=1\n",
      "                s.dk_counts[d,k] +=1\n",
      "                s.z.append(k)\n",
      "        s.ksum = s.dk_counts.sum(axis=0)\n",
      "\n",
      "    def samplePass(s):\n",
      "        i = 0 #count tokens\n",
      "        Veta = s.V*s.eta\n",
      "\n",
      "        for d,v,c in zip(s.text.row, s.text.col, s.text.data):\n",
      "            for _ in range(c):\n",
      "                # decrement counters to exclude w_{dn} and z_{dn}\n",
      "                k = s.z[i]\n",
      "                s.kw_counts[k,v] -= 1\n",
      "                s.dk_counts[d,k] -= 1\n",
      "                s.ksum[k] -= 1\n",
      "        \n",
      "                # sample\n",
      "                p_wz = (s.kw_counts[:,v] + s.eta) / (s.ksum + Veta)\n",
      "                p_zth = (s.dk_counts[d,:] + s.alpha) #don't need the denominator because it's the same for all topics\n",
      "                k = multiSample2(p_wz * p_zth)\n",
      "        \n",
      "                # increment counters              \n",
      "                s.z[i] = k\n",
      "                s.kw_counts[k,v] += 1\n",
      "                s.dk_counts[d,k] += 1\n",
      "                s.ksum[k] += 1\n",
      "                i += 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 96
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# this code prints the top words for each topic, according to tf-idf\n",
      "# vectorizer is a CountVectorizer object\n",
      "def printTopics(kw_counts,vectorizer,num_words=10):\n",
      "    idf = np.log(1e-4+kw_counts.sum()) - np.log(1e-4+kw_counts.sum(axis=0))\n",
      "    ksum = kw_counts.sum(axis=1)\n",
      "    for i,w_counts in enumerate(kw_counts):\n",
      "        tfidf = w_counts * idf\n",
      "        top_guys = tfidf.argsort()[-num_words:][::-1] \n",
      "        print i,ksum[i],\n",
      "        for guy in top_guys:\n",
      "            print vectorizer.get_feature_names()[guy],\n",
      "        print \"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 100
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# instantiate an LDA object with 10 topics\n",
      "lda = LDA(x,10,eta=1e-5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 98
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# run LDA\n",
      "# this will take a while, maybe even an hour or two depending on your computer\n",
      "stride = 10\n",
      "for i in range(100):\n",
      "    lda.samplePass()\n",
      "    print '.',\n",
      "    if i % stride == stride-1:\n",
      "        print ''\n",
      "        printTopics(lda.kw_topics,vectorizer)\n",
      "        print ''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'lda' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-1-25cdc20b5295>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mstride\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mlda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamplePass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m'.'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstride\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mstride\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mNameError\u001b[0m: name 'lda' is not defined"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 3a** Which of the topics make the most sense to you? Why? Which make less sense?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(your answer here)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 3b** For each topic, compute the probability of each subreddit given the topic."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Subreddit-specific LDA ##\n",
      "\n",
      "Note that we have a bit of a tension between finding the specific words that characterize each subreddit (in the classification section) and finding general topics that are shared across subreddits (in this section).\n",
      "\n",
      "We're now going to design a model that does both. We'll maintain word distributions for both latent topics as well as for subreddits. This model is similar to [Cross-Collection LDA](http://www.newdesign.aclweb.org/anthology/D/D09/D09-1146.pdf\u200e) by Paul and Girju. Besides the additional per-subreddit word distribution, we need an additional \"switch\" variable to decide whether each word comes from a latent topic or from the subreddit word distribution."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here is the generative story.\n",
      "\n",
      "- $s_d$ is the subreddit for document $d$. This is always observed.\n",
      "- $\\beta_k \\sim \\text{Dirichlet}(\\eta)$ Topic word distributions\n",
      "- $\\phi_s \\sim \\text{Dirichlet}(\\eta)$ Subreddit word distributions\n",
      "- $\\theta_d \\sim \\text{Dirichlet}(\\alpha)$ Document topic proportions\n",
      "- $\\pi_d \\sim \\text{Dirichlet}(\\gamma)$ Document \"switch\" distribution\n",
      "- $z_{dn} \\sim \\text{Categorial}(\\theta_d)$ Per-word topic, $z_{dn} \\in \\{1,\\ldots,K\\}$\n",
      "- $x_{dn} \\sim \\text{Categorial}(\\pi_d)$ Per-word switch, $x_{dn} \\in \\{0,1\\}$\n",
      "- If $x_{dn} == 0$, then $w_{dn} \\sim \\text{Categorial}(\\beta_{z_{dn}})$\n",
      "- If $x_{dn} == 1$, then $w_{dn} \\sim \\text{Categorial}(\\phi_{s_d})$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here are the complete collapsed Gibbs sampling equations. \n",
      "\n",
      "- $P(x_{dn} = 0 | w_{dn} = i, z_{dn} = k, \\ldots) \\propto \n",
      "\\frac{N(x_{d,-n} = 0) + \\gamma}{N_d - 1 + 2\\gamma}\n",
      "\\frac{N(x_{d,-n} = 0, z_{d,-n} = k, w_{d,-n} = i) + \\eta}{N(x_{d,-n} = 0, z_{d,-n} = k) + V \\eta)}\n",
      "$\n",
      "- $P(x_{dn} = 1 | w_{dn} = i, s_d = j, \\ldots) \\propto \n",
      "\\frac{N(x_{d,-n} = 0) + \\gamma}{N_d - 1 + 2\\gamma}\n",
      "\\frac{N(x_{d,-n} = 0, s_d = j, w_{d,-n} = i) + \\eta}{N(x_{d,-n} = 0, s_{d} = j) + V \\eta)}\n",
      "$\n",
      "- $P(z_{dn} = k | w_{dn} = i, x_{dn} = 0, \\ldots) \\propto \n",
      "\\frac{N(z_{d,-n} = k) + \\alpha}{N_d - 1 + K\\alpha}\n",
      "\\frac{N(x_{d,-n} = 0, z_{d,-n} = k, w_{d,-n} = i) + \\eta}{N(x_{d,-n} = 0, z_{d,-n} = k) + V \\eta)}\n",
      "$\n",
      "- $P(z_{dn} = k | w_{dn} = i, x_{dn} = 1, \\ldots) \\propto \n",
      "\\frac{N(z_{d,-n} = k) + \\alpha}{N_d - 1 + K\\alpha}\n",
      "$\n",
      "\n",
      "Note that it would be possible to do a [blocked sampler](http://en.wikipedia.org/wiki/Gibbs_sampling#Blocked_Gibbs_sampler), which would sample both z and x at the same time.\n",
      "This might work substantially better, but I left it out to keep things simple.\n",
      "It's not much more work though, so consider it if you're interested."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 4a** Note that the fourth equation does not include a likelihood term for w. Why not?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(your answer here)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 4b** The implementation of collapsed Gibbs sampling for LDA above includes two arrays for storing counts, kw_counts and dk_counts. What data structures will you need to store counts in this new model?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(your answer here)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 4c** Implement the model, and apply it to your data. Use $\\gamma = 1, K=10$ (at least), and 100 iterations."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class RedditLDA():\n",
      "    def __init__(s,text,labels,K,gamma = 1,alpha = 1e-1, eta=1e-4):\n",
      "    def samplePass(s):"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# shuffle x; this can be helpful to make sampling work faster\n",
      "idxs = range(len(labels))\n",
      "shuffle(idxs)\n",
      "x_shuf = (x.tocsr()[idxs,:]).tocoo()\n",
      "labels_shuf = [labels[i] for i in idxs]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 103
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rlda = RedditLDA(x_shuf,labels_shuf,10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 104
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stride = 10\n",
      "for i in range(100):\n",
      "    rlda.samplePass()\n",
      "    print '.',\n",
      "    if i % stride == stride - 1:\n",
      "        print ''\n",
      "        # print subreddit words here\n",
      "        print ''\n",
      "        # print latent topics here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ". \n",
        "0 10299.0 government your "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "libertarian good libertarians "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "get should police "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "when gun \n",
        "1 9405.0 he "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "his obama "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "she wage "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "her had conservative "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "gay me \n",
        "2 11078.0 he "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "them news my "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "when up "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "being article how "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "should \n",
        "3 21900.0 socialism socialist "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "http gt class "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "com government www "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "venezuela workers \n",
        "4 18315.0 anarchist "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "anarchism my "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "anarchists some "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "me also "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "gt http "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "conspiracy \n",
        "\n",
        "0 5403.0 other "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "us your "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "http he "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "how should only "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "been were \n",
        "1 5732.0 my "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "then right than "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "even me our "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "http how much \n",
        "2 5768.0 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "my he government "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "state way up "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "were should still "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "these \n",
        "3 5581.0 re he "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "your going need "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "how than why "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "being only \n",
        "4 5500.0 up "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "out see really "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "other get which "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "his com "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "were \n",
        "5 5589.0 some "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "http say them "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "work he well "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "me right "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "him \n",
        "6 5765.0 out "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "some right "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "which any "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "may your "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "government how "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "should \n",
        "7 5713.0 my "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "way know "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "when such "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "http them "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "right very he \n",
        "8 5630.0 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "he good them "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "when its "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "here should any "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "get state \n",
        "9 5623.0 how "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "want when out "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "our them does "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "right up "
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-105-325cca592eb1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mstride\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mrlda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamplePass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m'.'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstride\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mstride\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-26-6c516cfc545d>\u001b[0m in \u001b[0;36msamplePass\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m                 \u001b[1;31m# sample\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m                 \u001b[0mp_x_prior\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdx_counts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m                 \u001b[0mp_z_prior\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdk_counts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "those \n"
       ]
      }
     ],
     "prompt_number": 105
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# "
     ]
    }
   ],
   "metadata": {}
  }
 ]
}